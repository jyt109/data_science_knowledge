{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluation Metrics\n",
    "\n",
    "###Workflow\n",
    "\n",
    "- Evaluate how \"well\" the model predicts the target\n",
    "- Usually the model is trained on a **train set**\n",
    "- Hyperparameters are tuned on a **dev set**\n",
    "- Model is evaluated on a **test set**\n",
    "- The ratio should be around 60:20:20\n",
    "- Evaluation done on the **test set** should be cross validated\n",
    "- After evaluation, the model is trained on the whole data set\n",
    "- When data is scarce, one can dispense with the **dev set** and evaluate directly on the **test set**\n",
    "\n",
    "<br>\n",
    "\n",
    "###Common Metrics\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- Receiever Operating Characteristics (ROC)\n",
    "- F1 Score\n",
    "- Entropy based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutinomial data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score\n",
    "\n",
    "# Multinomial\n",
    "actual    = np.array([0, 1, 2, 3, 0, 0, 2])\n",
    "predicted = np.array([0, 1, 1, 3, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Accuracy\n",
    "\n",
    "$$Accuracy = \\frac{\\text{# of correctly predicted samples}}{\\text{# of samples}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "def accuracy(actual, predicted):\n",
    "    return sum(actual == predicted) / (len(actual) * 1.)\n",
    "\n",
    "eval_acc = accuracy(actual, predicted)\n",
    "eval_acc_sk = accuracy_score(actual, predicted)\n",
    "assert eval_acc == eval_acc_sk\n",
    "\n",
    "print 'Accuracy:', eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "- **Not informative**\n",
    "  - One number that does not indicate which labels have been misclassified\n",
    "- **Does not take into account unequal misclassification cost for each label**\n",
    "  - Misclassifying some labels are more consequential than others\n",
    "- **Misleading in the case of class imbalance**\n",
    "  - Predict all non-fraud in fraud detection will achieve high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Recall\n",
    "\n",
    "- aka **sensitivity**\n",
    "- Fraction of positives recalled\n",
    "- Predicted positives divided by all the actual positives\n",
    "\n",
    "$$Recall = \\frac{\\text{True positives}}{\\text{True positives} + \\text{False negatives}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (overall): 0.571428571429\n",
      "Recall (average): 0.708333333333\n",
      "Recall (weighted): 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "def recall_per_label(actual, predicted, n=1): # n is the positive label\n",
    "    tp = ((predicted == n) & (actual == predicted)).sum()\n",
    "    fn = ((actual == n) & (actual != predicted)).sum()\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def recall(actual, predicted, method='weighted'):\n",
    "    if method == 'micro':\n",
    "        tp = (actual == predicted).sum()\n",
    "        fn = (actual != predicted).sum()\n",
    "        return tp / (tp + fn)\n",
    "    \n",
    "    if method in ['macro', 'weighted']:\n",
    "        uniq_nums = np.unique(actual)\n",
    "        recall_arr = np.array([recall_per_label(actual, predicted, n=n) for n in uniq_nums])\n",
    "        if method == 'macro':\n",
    "            return np.mean(recall_arr)\n",
    "        if method == 'weighted':\n",
    "            weights = np.bincount(actual)[uniq_nums] / len(actual)\n",
    "            return np.sum(recall_arr * weights)\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "\n",
    "eval_recall_overall = recall(actual, predicted, method='micro')\n",
    "eval_recall_avg = recall(actual, predicted, method='macro')\n",
    "eval_recall_weighted = recall(actual, predicted, method='weighted')\n",
    "\n",
    "eval_recall_overall_sk = recall_score(actual, predicted, average='micro')\n",
    "eval_recall_avg_sk = recall_score(actual, predicted, average='macro')\n",
    "eval_recall_weighted_sk = recall_score(actual, predicted, average='weighted')\n",
    "\n",
    "assert eval_recall_overall == eval_recall_overall_sk\n",
    "assert eval_recall_avg == eval_recall_avg_sk\n",
    "assert eval_recall_weighted == eval_recall_weighted_sk\n",
    "\n",
    "print 'Recall (overall):', eval_recall_overall \n",
    "print 'Recall (average):', eval_recall_avg\n",
    "print 'Recall (weighted):', eval_recall_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "- Only account for positive results\n",
    "  - Does not penalize false positives\n",
    "- Recall alone is a poor metric\n",
    "\n",
    "**Advantages**\n",
    "- Given multiple labels, can weight the recall by the true positives in each label\n",
    "  - Account for class imbalance, unlike vanilla version of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part 5: F1 Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
